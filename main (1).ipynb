{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b486564",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import imageio\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from models.nerf_mlp import NeRFMLP\n",
    "from utils.encoding import PositionalEncoding\n",
    "from utils.render import render_rays\n",
    "import matplotlib.cm as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0b2055a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rays(H, W, focal, c2w):\n",
    "    device = c2w.device\n",
    "    i, j = torch.meshgrid(torch.linspace(0, W - 1, W, device=device), torch.linspace(0, H - 1, H, device=device), indexing='xy')\n",
    "    dirs = torch.stack([(i - W * 0.5) / focal, -(j - H * 0.5) / focal, -torch.ones_like(i)], -1)\n",
    "    rays_d = torch.sum(dirs[..., None, :] * c2w[:3, :3], -1)\n",
    "    rays_o = c2w[:3, 3].expand(rays_d.shape)\n",
    "    return rays_o, rays_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96103ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(base_dir, n_iters, batch_size):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # --- Load data into CPU RAM first ---\n",
    "    print(\"Loading data into RAM...\")\n",
    "    with open(os.path.join(base_dir, 'transforms_train.json'), 'r') as f:\n",
    "        meta = json.load(f)\n",
    "    \n",
    "    images_np, poses_np = [], []\n",
    "    for frame in meta['frames']:\n",
    "        fname = os.path.join(base_dir, frame['file_path'] + '.png')\n",
    "        img_rgba = (np.array(Image.open(fname)) / 255.0).astype(np.float32)\n",
    "        img_rgb = img_rgba[..., :3] * img_rgba[..., 3:] + (1.0 - img_rgba[..., 3:])\n",
    "        images_np.append(img_rgb)\n",
    "        poses_np.append(np.array(frame['transform_matrix']))\n",
    "\n",
    "    images_np = np.stack(images_np)\n",
    "    poses_np = np.stack(poses_np)\n",
    "    H, W = images_np[0].shape[:2]\n",
    "    camera_angle_x = float(meta['camera_angle_x'])\n",
    "    focal = 0.5 * W / np.tan(0.5 * camera_angle_x)\n",
    "\n",
    "    # --- Pre-compute all rays and move the ENTIRE dataset to the GPU ---\n",
    "    print(\"Pre-computing all rays and moving dataset to GPU VRAM...\")\n",
    "    all_rays_o = []\n",
    "    all_rays_d = []\n",
    "    for pose in tqdm(poses_np, desc=\"Processing Poses\"):\n",
    "        pose_tensor = torch.tensor(pose, dtype=torch.float32, device=device)\n",
    "        rays_o, rays_d = get_rays(H, W, focal, pose_tensor)\n",
    "        all_rays_o.append(rays_o.reshape(-1, 3))\n",
    "        all_rays_d.append(rays_d.reshape(-1, 3))\n",
    "\n",
    "    # These now live permanently on the GPU\n",
    "    all_rays_o = torch.cat(all_rays_o, 0)\n",
    "    all_rays_d = torch.cat(all_rays_d, 0)\n",
    "    all_colors = torch.tensor(images_np.reshape(-1, 3), dtype=torch.float32, device=device)\n",
    "    print(\"Dataset is now fully in VRAM! üöÄ\")\n",
    "\n",
    "    # --- Models, Encoders, Optimizer (all on GPU) ---\n",
    "    pos_enc = PositionalEncoding(num_freqs=10).to(device)\n",
    "    dir_enc = PositionalEncoding(num_freqs=4).to(device)\n",
    "    models = {\n",
    "        'coarse': NeRFMLP(input_ch=pos_enc.output_dims, input_ch_dir=dir_enc.output_dims).to(device),\n",
    "        'fine': NeRFMLP(input_ch=pos_enc.output_dims, input_ch_dir=dir_enc.output_dims).to(device)\n",
    "    }\n",
    "    optimizer = torch.optim.Adam(list(models['coarse'].parameters()) + list(models['fine'].parameters()), lr=5e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9995)\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    # --- Ultra-Fast Training Loop (No DataLoader) ---\n",
    "    print(\"\\nStarting training...\")\n",
    "    num_total_rays = all_rays_o.shape[0]\n",
    "    for i in tqdm(range(n_iters), desc=\"Training Progress\"):\n",
    "        # 1. Generate random indices directly on the GPU\n",
    "        indices = torch.randint(0, num_total_rays, (batch_size,), device=device)\n",
    "\n",
    "        # 2. Gather a batch using the indices (very fast GPU-to-GPU copy)\n",
    "        batch_rays_o = all_rays_o[indices]\n",
    "        batch_rays_d = all_rays_d[indices]\n",
    "        batch_colors = all_colors[indices]\n",
    "\n",
    "        # 3. Perform the training step\n",
    "        with autocast():\n",
    "            results = render_rays(\n",
    "                models, pos_enc, dir_enc, batch_rays_o, batch_rays_d,\n",
    "                N_samples=64, N_importance=128, near=2.0, far=6.0, \n",
    "                white_bkgd=True, device=device\n",
    "            )\n",
    "            loss = torch.mean((results['rgb_map'] - batch_colors) ** 2) + torch.mean((results['rgb_map0'] - batch_colors) ** 2)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "    print(\"Training finished! ‚úÖ\")\n",
    "    # ... (return models and other variables for video rendering) ...\n",
    "    torch.save({\n",
    "        'coarse_model_state_dict': models['coarse'].state_dict(),\n",
    "        'fine_model_state_dict': models['fine'].state_dict(),\n",
    "    }, 'nerf_models.pth')\n",
    "    print(\"Saved trained models to nerf_models.pth\")\n",
    "\n",
    "    # --- THE FIX: Add the return statement ---\n",
    "    return models, H, W, focal, pos_enc, dir_enc, device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3632bc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_camera_path(radius=4.0, n_poses=120):\n",
    "    \"\"\"Creates a circular camera orbit.\"\"\"\n",
    "    c2w_frames = []\n",
    "    angles = torch.linspace(0, 2 * np.pi, n_poses + 1)[:-1]\n",
    "    for theta in angles:\n",
    "        # Camera position\n",
    "        pos = torch.tensor([\n",
    "            radius * np.cos(theta), \n",
    "            -0.5,  # Slight downward tilt\n",
    "            radius * np.sin(theta)\n",
    "        ])\n",
    "        \n",
    "        # Look at origin (where the object is)\n",
    "        target = torch.tensor([0., 0., 0.])\n",
    "        forward = target - pos\n",
    "        forward = forward / torch.linalg.norm(forward)\n",
    "        \n",
    "        # Define up direction\n",
    "        world_up = torch.tensor([0., 1., 0.])\n",
    "        right = torch.cross(world_up, forward)\n",
    "        right = right / torch.linalg.norm(right)\n",
    "        up = torch.cross(forward, right)\n",
    "        \n",
    "        # Build camera-to-world matrix\n",
    "        c2w = torch.eye(4)\n",
    "        c2w[:3, 0] = right\n",
    "        c2w[:3, 1] = up\n",
    "        c2w[:3, 2] = forward\n",
    "        c2w[:3, 3] = pos\n",
    "        \n",
    "        c2w_frames.append(c2w[:3, :4])  # Only need 3x4\n",
    "    \n",
    "    return torch.stack(c2w_frames)\n",
    "\n",
    "def create_nerf_video(models, H, W, focal, pos_enc, dir_enc, device):\n",
    "    \"\"\"Renders a video from the trained models.\"\"\"\n",
    "    print(\"\\nStarting video rendering...\")\n",
    "    camera_poses = create_camera_path().to(device)\n",
    "    frames = []\n",
    "\n",
    "    for model in models.values():\n",
    "        model.eval()\n",
    "\n",
    "    for pose in tqdm(camera_poses, desc=\"Rendering Frames\"):\n",
    "        with torch.no_grad():\n",
    "            rays_o, rays_d = get_rays(H, W, focal, pose)\n",
    "            rays_o, rays_d = rays_o.reshape(-1, 3), rays_d.reshape(-1, 3)\n",
    "\n",
    "            all_rgb = []\n",
    "            for i in range(0, rays_o.shape[0], 4096):\n",
    "                results = render_rays(\n",
    "                    models, pos_enc, dir_enc, rays_o[i:i+4096], rays_d[i:i+4096],\n",
    "                    N_samples=64, N_importance=128, near=2.0, far=6.0, \n",
    "                    white_bkgd=True, device=device\n",
    "                )\n",
    "                all_rgb.append(results['rgb_map'])\n",
    "            \n",
    "            full_image = torch.cat(all_rgb, 0).reshape(H, W, 3)\n",
    "            img_np = (full_image.cpu().numpy() * 255).astype(np.uint8)\n",
    "            frames.append(img_np)\n",
    "    \n",
    "    video_path = 'nerf_lego_video.mp4'\n",
    "    \n",
    "    # === FIX: Use explicit FFMPEG writer ===\n",
    "    try:\n",
    "        import imageio.v3 as iio\n",
    "        iio.imwrite(video_path, frames, fps=30, codec='libx264', quality=8)\n",
    "        print(f\"Video saved to {video_path}! üéâ\")\n",
    "    except Exception as e:\n",
    "        print(f\"MP4 save failed: {e}\")\n",
    "        print(\"Falling back to GIF format...\")\n",
    "        gif_path = 'nerf_lego_video.gif'\n",
    "        imageio.mimsave(gif_path, frames, fps=30)\n",
    "        print(f\"GIF saved to {gif_path}! üéâ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7fe9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_combined_video(models, H, W, focal, pos_enc, dir_enc, device):\n",
    "    \"\"\"Creates a single video with RGB | Opacity | Depth side-by-side\"\"\"\n",
    "    print(\"\\nRendering combined visualization...\")\n",
    "    camera_poses = create_camera_path().to(device)\n",
    "    combined_frames = []\n",
    "\n",
    "    for model in models.values():\n",
    "        model.eval()\n",
    "\n",
    "    import matplotlib.cm as cm\n",
    "    \n",
    "    for pose in tqdm(camera_poses, desc=\"Rendering\"):\n",
    "        with torch.no_grad():\n",
    "            rays_o, rays_d = get_rays(H, W, focal, pose)\n",
    "            rays_o, rays_d = rays_o.reshape(-1, 3), rays_d.reshape(-1, 3)\n",
    "\n",
    "            all_rgb, all_acc, all_depth = [], [], []\n",
    "            \n",
    "            for i in range(0, rays_o.shape[0], 4096):\n",
    "                results = render_rays(\n",
    "                    models, pos_enc, dir_enc, rays_o[i:i+4096], rays_d[i:i+4096],\n",
    "                    N_samples=64, N_importance=128, near=2.0, far=6.0, \n",
    "                    white_bkgd=True, device=device\n",
    "                )\n",
    "                all_rgb.append(results['rgb_map'])\n",
    "                all_acc.append(results['acc_map'])\n",
    "                all_depth.append(results['depth_map'])\n",
    "            \n",
    "            # Process each view\n",
    "            rgb = (torch.cat(all_rgb, 0).reshape(H, W, 3).cpu().numpy() * 255).astype(np.uint8)\n",
    "            \n",
    "            acc = torch.cat(all_acc, 0).reshape(H, W).cpu().numpy()\n",
    "            acc_colored = (cm.hot(acc) * 255).astype(np.uint8)[..., :3]\n",
    "            \n",
    "            depth = torch.cat(all_depth, 0).reshape(H, W).cpu().numpy()\n",
    "            depth_norm = (depth - depth.min()) / (depth.max() - depth.min() + 1e-8)\n",
    "            depth_colored = (cm.viridis(depth_norm) * 255).astype(np.uint8)[..., :3]\n",
    "            \n",
    "            # Stack horizontally: [RGB | Opacity | Depth]\n",
    "            combined = np.hstack([rgb, acc_colored, depth_colored])\n",
    "            combined_frames.append(combined)\n",
    "    \n",
    "    # Save\n",
    "    try:\n",
    "        import imageio.v3 as iio\n",
    "        iio.imwrite('nerf_COMBINED.mp4', combined_frames, fps=30, codec='libx264', quality=8)\n",
    "        print(\"üéâ Saved nerf_COMBINED.mp4 (RGB | Opacity | Depth)\")\n",
    "    except:\n",
    "        imageio.mimsave('nerf_COMBINED.gif', combined_frames, fps=30)\n",
    "        print(\"üéâ Saved nerf_COMBINED.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f848ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================\n",
    "# === MAIN EXECUTION BLOCK (MODIFIED FOR RENDERING) ===============================\n",
    "# =================================================================================\n",
    "\n",
    "def opacity_diagnostic(models, H, W, focal, pos_enc, dir_enc, device):\n",
    "    \"\"\"Check if model is producing any solid geometry\"\"\"\n",
    "    print(\"\\nüî¨ OPACITY DIAGNOSTIC\\n\" + \"=\"*50)\n",
    "    \n",
    "    with open(os.path.join(LEGO_DATA_DIR, 'transforms_train.json'), 'r') as f:\n",
    "        meta = json.load(f)\n",
    "    \n",
    "    pose = torch.tensor(meta['frames'][0]['transform_matrix'], dtype=torch.float32, device=device)\n",
    "    rays_o, rays_d = get_rays(H, W, focal, pose)\n",
    "    rays_o, rays_d = rays_o.reshape(-1, 3), rays_d.reshape(-1, 3)\n",
    "    \n",
    "    for model in models.values():\n",
    "        model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Render full image\n",
    "        all_rgb = []\n",
    "        all_acc = []\n",
    "        \n",
    "        for i in range(0, rays_o.shape[0], 4096):\n",
    "            results = render_rays(\n",
    "                models, pos_enc, dir_enc, rays_o[i:i+4096], rays_d[i:i+4096],\n",
    "                N_samples=64, N_importance=128, near=2.0, far=6.0, \n",
    "                white_bkgd=False,  # No background to see raw output\n",
    "                device=device\n",
    "            )\n",
    "            all_rgb.append(results['rgb_map'])\n",
    "            all_acc.append(results['acc_map'])\n",
    "        \n",
    "        rgb_map = torch.cat(all_rgb, 0).reshape(H, W, 3).cpu().numpy()\n",
    "        acc_map = torch.cat(all_acc, 0).reshape(H, W).cpu().numpy()\n",
    "    \n",
    "    print(f\"RGB (no background):\")\n",
    "    print(f\"  Min: {rgb_map.min():.4f} | Max: {rgb_map.max():.4f} | Mean: {rgb_map.mean():.4f}\")\n",
    "    print(f\"\\nOpacity Map (acc_map):\")\n",
    "    print(f\"  Min: {acc_map.min():.4f} | Max: {acc_map.max():.4f} | Mean: {acc_map.mean():.4f}\")\n",
    "    \n",
    "    # Diagnosis\n",
    "    if acc_map.mean() < 0.3:\n",
    "        print(\"\\n‚ùå PROBLEM: Very low opacity (mean < 0.3)\")\n",
    "        print(\"   ‚Üí Model outputs weak density everywhere\")\n",
    "        print(\"   ‚Üí Need to boost sigma output or retrain longer\")\n",
    "    elif acc_map.mean() > 0.95:\n",
    "        print(\"\\n‚ö†Ô∏è  Very high opacity everywhere (mean > 0.95)\")\n",
    "        print(\"   ‚Üí Model might be outputting noise\")\n",
    "    else:\n",
    "        print(\"\\n‚úÖ Opacity looks reasonable!\")\n",
    "    \n",
    "    # Visualize\n",
    "    import matplotlib.pyplot as plt\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    axes[0].imshow(rgb_map)\n",
    "    axes[0].set_title('RGB (No Background)')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(acc_map, cmap='hot')\n",
    "    axes[1].set_title('Opacity Map')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # With white background\n",
    "    rgb_with_bg = rgb_map + (1 - acc_map[..., None])\n",
    "    axes[2].imshow(np.clip(rgb_with_bg, 0, 1))\n",
    "    axes[2].set_title('With White Background')\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('full_diagnostic.png', dpi=150, bbox_inches='tight')\n",
    "    print(\"\\nüíæ Saved full_diagnostic.png\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "def detailed_diagnostic(models, H, W, focal, pos_enc, dir_enc, device):\n",
    "    \"\"\"Deep dive into what the model is producing\"\"\"\n",
    "    print(\"\\nüî¨ DETAILED DIAGNOSTIC\\n\" + \"=\"*50)\n",
    "    \n",
    "    with open(os.path.join(LEGO_DATA_DIR, 'transforms_train.json'), 'r') as f:\n",
    "        meta = json.load(f)\n",
    "    \n",
    "    # Use first training pose\n",
    "    pose = torch.tensor(meta['frames'][0]['transform_matrix'], dtype=torch.float32, device=device)\n",
    "    rays_o, rays_d = get_rays(H, W, focal, pose)\n",
    "    rays_o, rays_d = rays_o.reshape(-1, 3), rays_d.reshape(-1, 3)\n",
    "    \n",
    "    for model in models.values():\n",
    "        model.eval()\n",
    "    \n",
    "    # Sample just center of image\n",
    "    center_idx = rays_o.shape[0] // 2\n",
    "    sample_rays_o = rays_o[center_idx:center_idx+4096]\n",
    "    sample_rays_d = rays_d[center_idx:center_idx+4096]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        results = render_rays(\n",
    "            models, pos_enc, dir_enc, sample_rays_o, sample_rays_d,\n",
    "            N_samples=64, N_importance=128, near=2.0, far=6.0, \n",
    "            white_bkgd=True, device=device\n",
    "        )\n",
    "    \n",
    "    rgb = results['rgb_map'].cpu().numpy()\n",
    "    \n",
    "    print(f\"RGB Stats:\")\n",
    "    print(f\"  Min: {rgb.min():.4f} | Max: {rgb.max():.4f} | Mean: {rgb.mean():.4f}\")\n",
    "    \n",
    "    # NOW THE KEY DIAGNOSTIC - check opacity\n",
    "    # We need to modify render_rays to return acc_map first...\n",
    "    # Let's do a manual test\n",
    "    \n",
    "    # Test with NO white background\n",
    "    with torch.no_grad():\n",
    "        results_no_bg = render_rays(\n",
    "            models, pos_enc, dir_enc, sample_rays_o, sample_rays_d,\n",
    "            N_samples=64, N_importance=128, near=2.0, far=6.0, \n",
    "            white_bkgd=False,  # ‚Üê KEY CHANGE\n",
    "            device=device\n",
    "        )\n",
    "    \n",
    "    rgb_no_bg = results_no_bg['rgb_map'].cpu().numpy()\n",
    "    print(f\"\\nRGB WITHOUT white background:\")\n",
    "    print(f\"  Min: {rgb_no_bg.min():.4f} | Max: {rgb_no_bg.max():.4f} | Mean: {rgb_no_bg.mean():.4f}\")\n",
    "    \n",
    "    # If mean is very low without background, density is the problem\n",
    "    if rgb_no_bg.mean() < 0.1:\n",
    "        print(\"\\n‚ö†Ô∏è  DIAGNOSIS: Model has VERY LOW DENSITY\")\n",
    "        print(\"   The model barely learned any solid geometry!\")\n",
    "    \n",
    "    # Save comparison\n",
    "    import matplotlib.pyplot as plt\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    ax1.imshow(rgb.reshape(64, 64, 3))\n",
    "    ax1.set_title('With White BG')\n",
    "    ax2.imshow(rgb_no_bg.reshape(64, 64, 3))\n",
    "    ax2.set_title('Without BG (Raw)')\n",
    "    plt.savefig('diagnostic_comparison.png', dpi=150, bbox_inches='tight')\n",
    "    print(\"\\nüíæ Saved diagnostic_comparison.png\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "# Call this instead of quick_diagnostic\n",
    "\n",
    "# Run this after loading model\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    LEGO_DATA_DIR = './data/nerf_synthetic/lego'\n",
    "    BATCH_SIZE = 1024\n",
    "    TRAIN_MODEL = False  # Keep as False since you have trained model\n",
    "\n",
    "    if not TRAIN_MODEL:\n",
    "        print(\"Loading saved model...\")\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        with open(os.path.join(LEGO_DATA_DIR, 'transforms_train.json'), 'r') as f:\n",
    "            meta = json.load(f)\n",
    "        H, W = 800, 800\n",
    "        camera_angle_x = float(meta['camera_angle_x'])\n",
    "        focal = 0.5 * W / np.tan(0.5 * camera_angle_x)\n",
    "\n",
    "        pos_enc = PositionalEncoding(num_freqs=10).to(device)\n",
    "        dir_enc = PositionalEncoding(num_freqs=4).to(device)\n",
    "        trained_models = {\n",
    "            'coarse': NeRFMLP(input_ch=pos_enc.output_dims, input_ch_dir=dir_enc.output_dims).to(device),\n",
    "            'fine': NeRFMLP(input_ch=pos_enc.output_dims, input_ch_dir=dir_enc.output_dims).to(device)\n",
    "        }\n",
    "\n",
    "        checkpoint = torch.load('nerf_models.pth', map_location=device)\n",
    "        trained_models['coarse'].load_state_dict(checkpoint['coarse_model_state_dict'])\n",
    "        trained_models['fine'].load_state_dict(checkpoint['fine_model_state_dict'])\n",
    "        print(\"‚úÖ Models loaded!\")\n",
    "\n",
    "        # Render the combined video (BEST OPTION)\n",
    "        create_combined_video(trained_models, H, W, focal, pos_enc, dir_enc, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
